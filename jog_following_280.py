#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Vision-based Follow, Pick & Place Demo for myCobot-280 (USB connection)
----------------------------------------------------------------------
Features:
1. Two-thread architecture identical to `jog_following_new.py` but
   using `MyCobot280` (serial/USB) instead of socket.
2. Homography (camera pixels ➜ robot X-Y) is loaded from
   `env/homography.json` generated by `calibrate_homography.py`.
3. Connection parameters (serial port & baud rate) can be supplied via
   environment variables or CLI flags.

Usage examples
--------------
$ python jog_following_280.py                     # auto-detect /dev/ttyUSB0
$ python jog_following_280.py --port COM3 --baud 115200  # Windows example

Keyboard controls inside OpenCV window:
  Enter  – finish following and start pick-and-place.
  q      – abort following without pick-and-place.
  ESC    – emergency abort (releases all servos).

"""

import argparse
import json
import os
import signal
import sys
import threading
import time
from pathlib import Path

import cv2
import numpy as np
from pymycobot import MyCobot280

# Shared robot pose targets
from robot_constants import TARGET_Z, TARGET_ORIENTATION

# ---------------------------- Utility functions ---------------------------- #

def load_homography(path: Path) -> np.ndarray:
    """Load 3×3 homography matrix from a JSON file.

    Expects the file to hold ``{"H": [[...],[...],[...]]}``. If the file is
    missing or malformed, returns identity and prints a warning.
    """
    try:
        with path.open() as f:
            data = json.load(f)
        H = np.array(data["H"], dtype=float)
        if H.shape != (3, 3):
            raise ValueError
        return H
    except Exception as exc:  # noqa: BLE001
        print(
            f"[WARN] Could not load homography from {path} — {exc}. "
            "Using identity; tracking will be wrong until you calibrate."
        )
        return np.eye(3, dtype=float)


def convert_camera_to_robot(camera_coord: tuple[float, float], H: np.ndarray) -> tuple[float, float]:
    """Map a 2-D camera point (u,v) to robot plane (x,y) using homography."""
    u, v = camera_coord
    point_h = np.array([u, v, 1.0])
    robot_h = H @ point_h
    robot_h /= robot_h[2]
    return float(robot_h[0]), float(robot_h[1])


# ----------------------- Image-processing thread --------------------------- #

global_target: tuple[float, float] | None = None  # shared between threads
confirm_follow = False
_target_lock = threading.Lock()

def image_processing_thread(H: np.ndarray, cam_index: int):
    global global_target, confirm_follow

    cap = cv2.VideoCapture(cam_index)
    if not cap.isOpened():
        print(f"[ERR] Unable to open camera index {cam_index}")
        confirm_follow = True
        return

    # HSV range for yellow (tweak if lighting differs)
    lower_yellow = np.array([20, 100, 100])
    upper_yellow = np.array([30, 255, 255])

    display_ok = True  # OpenCV imshow may fail on macOS when used from a non-main thread

    while not confirm_follow:
        ret, frame = cap.read()
        if not ret:
            print("[ERR] Unable to read frame, exiting image thread…")
            break

        hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)
        mask = cv2.inRange(hsv, lower_yellow, upper_yellow)
        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

        if contours:
            largest = max(contours, key=cv2.contourArea)
            if cv2.contourArea(largest) > 500:
                x, y, w, h = cv2.boundingRect(largest)
                cx, cy = x + w // 2, y + h // 2
                cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)
                cv2.circle(frame, (cx, cy), 5, (0, 0, 255), -1)
                cv2.putText(frame, f"({cx}, {cy})", (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)

                robot_xy = convert_camera_to_robot((cx, cy), H)
                with _target_lock:
                    global_target = robot_xy

        if display_ok:
            try:
                cv2.imshow("Yellow Object", frame)
                cv2.imshow("Mask", mask)
            except cv2.error as exc:
                # Likely running headless or imshow from thread not supported.
                print("[WARN] cv2.imshow failed (", exc, ") — switching to headless mode.")
                display_ok = False

                # Spawn a helper thread waiting for Enter in the terminal.
                def _stdin_wait():  # noqa: D401
                    input("[HEADLESS] Press Enter in the terminal to confirm target and start pick-and-place…\n")
                    global confirm_follow  # noqa: WPS503
                    confirm_follow = True

                threading.Thread(target=_stdin_wait, daemon=True).start()

        key = cv2.waitKey(1) & 0xFF if display_ok else 255
        if key in (13, ord("q")):
            confirm_follow = True

    cap.release()
    if display_ok:
        cv2.destroyAllWindows()


# ------------------------------ Main logic -------------------------------- #

def main():  # noqa: C901  # complexity OK for script level
    parser = argparse.ArgumentParser(description="Yellow-object follow and pick for myCobot-280")
    parser.add_argument("--port", default=os.getenv("MYCOBOT_PORT", "/dev/ttyUSB0"), help="Serial port, e.g. /dev/ttyUSB0 or COM3")
    parser.add_argument("--baud", type=int, default=int(os.getenv("MYCOBOT_BAUD", 115200)), help="Baud rate")
    parser.add_argument("--homography", default="env/homography.json", help="Path to homography JSON file")
    parser.add_argument("--cam-index", type=int, default=int(os.getenv("CAM_INDEX", 0)), help="OpenCV camera index (default 0). Use 1, 2… to select e.g. 'Android Webcam'.")
    args = parser.parse_args()

    H = load_homography(Path(args.homography))

    # Connect to robot
    try:
        mc = MyCobot280(args.port, args.baud)
    except Exception as exc:  # noqa: BLE001
        print(f"[ERR] Failed to connect to myCobot-280 on {args.port}: {exc}")
        sys.exit(1)

    # Graceful exit on Ctrl-C
    def _handle_sigint(_sig, _frame):  # noqa: D401
        print("\n[INFO] SIGINT received — exiting (servos remain enabled)…")
        sys.exit(0)

    signal.signal(signal.SIGINT, _handle_sigint)

    time.sleep(1)
    mc.focus_all_servos()
    time.sleep(0.5)
    mc.set_fresh_mode(1)  # execute commands immediately
    time.sleep(0.2)

    # myCobot-280's gripper does not need a separate "mode" switch; simply
    # ensure it starts in the open position. (320 uses set_gripper_mode.)
    print("\n[INFO] Initialising gripper (opening) …")
    mc.set_gripper_state(0, 100)
    time.sleep(1)

    home_angles = [0, 0, 0, 0, 0, 0]
    print("[INFO] Moving to home position …")
    mc.send_angles(home_angles, 30)
    time.sleep(2.5)

    # Gripper already opened above; no need to repeat unless desired.

    # Params (tweak for your setup)
    pick_z = TARGET_Z  # mm, height of TCP above table
    # Orientation: keep wrist vertical, yaw neutral (0°) to avoid cable twist.
    pick_orientation = TARGET_ORIENTATION.copy()  # roll, pitch, yaw
    speed = 100

    # Start vision thread
    img_thread = threading.Thread(target=image_processing_thread, args=(H, args.cam_index), daemon=True)
    img_thread.start()

    last_sent: tuple[float, float] | None = None
    threshold = 0.5  # mm
    max_step = 80.0  # mm per iteration

    try:
        while not confirm_follow:
            with _target_lock:
                target_xy = global_target
            if target_xy is not None:
                if last_sent is None:
                    last_sent = target_xy
                dx = target_xy[0] - last_sent[0]
                dy = target_xy[1] - last_sent[1]
                if abs(dx) >= threshold or abs(dy) >= threshold:
                    inc_x = float(max(-max_step, min(max_step, dx)))
                    inc_y = float(max(-max_step, min(max_step, dy)))
                    new_x = last_sent[0] + inc_x
                    new_y = last_sent[1] + inc_y
                    coords = [new_x, new_y, pick_z, *pick_orientation]
                    print(f"[INFO] Moving to coords: {coords}")
                    mc.send_coords(coords, speed, 1)
                    last_sent = (new_x, new_y)
            time.sleep(0.01)
    finally:
        img_thread.join()

    if last_sent is None:
        print("[WARN] No target tracked; exiting …")
        return

    # Pick & place sequence
    pick_coords = [last_sent[0], last_sent[1], pick_z, *pick_orientation]
    print("[INFO] Final pick coords:", pick_coords)
    time.sleep(0.5)

    print("[INFO] Closing gripper …")
    mc.set_gripper_state(1, 100)
    time.sleep(1.5)

    ascend_coords = [pick_coords[0], pick_coords[1], pick_z + 50, *pick_orientation]
    print("[INFO] Lifting object …")
    mc.send_coords(ascend_coords, speed, 1)
    time.sleep(2)

    place_coords = [-200.0, 80.0, 170.0, -180.0, -6.0, 0.0]  # yaw 0° to match pick_orientation
    print("[INFO] Moving to place coords …")
    mc.send_coords(place_coords, speed, 1)
    time.sleep(2)

    print("[INFO] Opening gripper to release …")
    mc.set_gripper_state(0, 100)
    time.sleep(1)

    print("[INFO] Returning home …")
    mc.send_angles(home_angles, 30)
    time.sleep(2.5)

    print("[INFO] Closing gripper (idle state) …")
    mc.set_gripper_state(1, 100)
    time.sleep(0.5)

    print("[SUCCESS] Pick & place complete. Enjoy!\n")


if __name__ == "__main__":
    main() 